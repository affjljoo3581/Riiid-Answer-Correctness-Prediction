type: BERT

datasets:
  vocab_path:           ../datasets/vocab.txt
  train_dataset_path:   ../datasets/train.txt
  val_dataset_path:     ../datasets/val.txt

  num_workers:          6

  min_seq_len:          16
  max_seq_len:          128

model:
  num_layers:           6
  num_heads:            8

  hidden_dims:          512
  bottleneck:           4

  dropout_rate:         0.1

training:
  train_batch_size:     256
  val_batch_size:       512

  base_lr:              0.001
  weight_decay:         0.01
  mlm_ratio:            0.15

  total_steps:          100000
  val_steps:            5000

  use_amp:              True
  clip_grad_norm:       0.5
  num_gpus:             1
